## 概述

全连接的神经网络使用全连接层（Affine 层）将相邻层的神经单元连接起来，输出的数量可以任意决定。但是当输入数据不是一维数据时（例如图像是高、宽、通道方向上的三维数据），全连接层会将其拉平为一维数据，这样就损失了输入数据中包含的空间信息。

而卷积层可以保持输入数据空间结构不变，并以同样的空间结构输出至下一层，这样可以更正确地理解输入数据。

## 原理

### 参数和变量

-   $x_{ij}$：输入层第$i$行第$j$列的输入值，与输出值$a^I_{ij}$相同；
-   $w^{F_k}_{ij}$：用于建立第$k$个特征映射的过滤器的第$i$行第$j$列的值；
-   $z^{F_k}_{ij}$：卷积层第$k$个子层的第$i$行第$j$列的神经单元的加权输入；
-   $b^{F_k}$：卷积层第$k$个子层的所有神经单元的偏置；
-   $a^{F_k}_{ij}$：卷积层第$k$个子层的第$i$行第$j$列的神经单元的输出（激活函数的值）；
-   $z^{P_k}_{ij}$：池化层第$k$个子层的第$i$行第$j$列的神经单元的输入，通常是对应卷积层输出值的非线性函数值；
-   $a^{P_k}_{ij}$：池化层第$k$个子层的第$i$行第$j$列的神经单元的输出，与输入值$z^{P_k}_{ij}$一致；
-   $w^{O_n}_{k-ij}$：从池化层第$k$个子层第$i$行第$j$列的神经单元指向输出层第$n$个神经单元的权重；
-   $z^O_i$：输出层第$i$个神经单元的加权输入；
-   $b^O_i$：输出层第$i$个神经单元的偏置；
-   $a^O_i$：输出层第$i$个神经单元的输出（激活函数的值）；
-   $t_i$：学习数据的第$i$个正解。

### 卷积 Convolution

设过滤器（卷积核）为$m\times n$的矩阵，则卷积层第$k$个子层的第$i$行第$j$列的卷积值（即相似度）如下，

$$
\begin{equation}
    c^{F_k}_{ij}=\sum_{r=1}^{m}\sum_{c=1}^{n} w^{F_k}_{rc}x_{r+i-1,c+j-1}
\end{equation}
$$

设激活函数为$a(z)$，则其输出值如下，

$$
\begin{equation}
    a^{F_k}_{ij}=a(z^{F_k}_{ij})=a(c^{F_k}_{ij}+b^{F_k})
\end{equation}
$$

依次滑动卷积核，即可得到使用任一过滤器的卷积的结果。其矩阵表示为，

$$
    A^{F_k}_{}=X\circledast W^{F_k}+b^{F_k}
$$

### 填充和步幅

在进行卷积层的处理前，先向输入数据的周围填入固定值（比如 0），以保证每次卷积后图像不会收缩，这就是填充（padding）。

应用滤波器的位置间隔称为步幅（stride）。

设输入数据和滤波器的大小分别为 $(m_{in},n_{in})$ 和 $(m,n)$，填充和步幅分别为 $p$ 和 $s$，则输出大小可以通过如下公式计算，

$$
\begin{gather}
    m_{out} = \frac{m_{in}+2p-m}{s}+1 \\
    n_{out} = \frac{n_{in}+2p-n}{s}+1
\end{gather}
$$

### 池化 Pooling

池化层用于压缩卷积层的信息，本质上也是神经单元的集合，但是计算输入时没有权重和偏置参数，也没有激活函数（或认为是$a(z)=z$），即输入值和输出值是相同的。

如果使用最大池化法，则有，

$$
\begin{equation}
    a^{P_k}_{ij}=z^{P_k}_{ij}=\max(a^{P_k}_{2i-1,2j-1},a^{P_k}_{2i-1,2j},a^{P_k}_{2i,2j-1},a^{P_k}_{2i,2j})
\end{equation}
$$

### 输出层

输出层第$n$个神经单元的加权输入如下，

$$
\begin{equation}
    z^O_n=\sum_k\sum_{i,j}w^{O_n}_{k-ij}a^{P_k}_{ij}+b^O_n
\end{equation}
$$

设激活函数为$a(z)$，则其输出值如下，

$$
\begin{equation}
    a^O_n=a(z^O_n)
\end{equation}
$$

CNN 中各层间传递的数据的四维数据，其形状为（批处理，通道，高，宽）。

### 代价函数

设$t$为正解，$k$为学习实例序号，则有代价函数如下，

$$
\begin{equation}
    C_T=\sum_{k}\sum_j(t_j-a^L_j)^2
\end{equation}
$$

## 参考

-   涌井良幸, 涌井贞美. 深度学习的数学.
-   斋藤康毅. 深度学习入门：基于 Python 的理论与实现.
