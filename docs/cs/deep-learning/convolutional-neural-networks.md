### 原理

#### 参数和变量

- $x_{ij}$：输入层第$i$行第$j$列的输入值，与输出值$a^I_{ij}$相同；
- $w^{F_k}_{ij}$：用于建立第$k$个特征映射的过滤器的第$i$行第$j$列的值；
- $z^{F_k}_{ij}$：卷积层第$k$个子层的第$i$行第$j$列的神经单元的加权输入；
- $b^{F_k}$：卷积层第$k$个子层的所有神经单元的偏置；
- $a^{F_k}_{ij}$：卷积层第$k$个子层的第$i$行第$j$列的神经单元的输出（激活函数的值）；
- $z^{P_k}_{ij}$：池化层第$k$个子层的第$i$行第$j$列的神经单元的输入，通常是对应卷积层输出值的非线性函数值；
- $a^{P_k}_{ij}$：池化层第$k$个子层的第$i$行第$j$列的神经单元的输出，与输入值$z^{P_k}_{ij}$一致；
- $w^{O_n}_{k-ij}$：从池化层第$k$个子层第$i$行第$j$列的神经单元指向输出层第$n$个神经单元的权重；
- $z^O_i$：输出层第$i$个神经单元的加权输入；
- $b^O_i$：输出层第$i$个神经单元的偏置；
- $a^O_i$：输出层第$i$个神经单元的输出（激活函数的值）；
- $t_i$：学习数据的第$i$个正解。

#### 卷积

设过滤器（卷积核）为$m\times n$的矩阵，则卷积层第$k$个子层的第$i$行第$j$列的卷积值（即相似度）如下，

$$
\begin{equation}
    c^{F_k}_{ij}=\sum_{1\le r\le m,1\le c\le n} w^{F_k}_{rc}x_{r+i-1,c+j-1}
\end{equation}
$$

设激活函数为$a(z)$，则其输出值如下，

$$
\begin{equation}
    a^{F_k}_{ij}=a(z^{F_k}_{ij})=a(c^{F_k}_{ij}+b^{F_k})
\end{equation}
$$

依次滑动卷积核，即可得到使用任一过滤器的卷积的结果。

#### 池化

池化层用于压缩卷积层的信息，本质上也是神经单元的集合，但是计算输入时没有权重和偏置参数，也没有激活函数（或认为是$a(z)=z$），即输入值和输出值是相同的。

如果使用最大池化法，则有，

$$
\begin{equation}
    a^{P_k}_{ij}=z^{P_k}_{ij}=\max(a^{P_k}_{2i-1,2j-1},a^{P_k}_{2i-1,2j},a^{P_k}_{2i,2j-1},a^{P_k}_{2i,2j})
\end{equation}
$$

#### 输出层

输出层第$n$个神经单元的加权输入如下，

$$
\begin{equation}
    z^O_n=\sum_k\sum_{i,j}w^{O_n}_{k-ij}a^{P_k}_{ij}+b^O_n
\end{equation}
$$

设激活函数为$a(z)$，则其输出值如下，

$$
\begin{equation}
    a^O_n=a(z^O_n)
\end{equation}
$$

#### 代价函数

设$t$为正解，$k$为学习实例序号，则有代价函数如下，

$$
\begin{equation}
    C_T=\sum_{k}\sum_j(t_j-a^L_j)^2
\end{equation}
$$

#### 数字照片的数据结构

图像实际上就是巨大的数字矩阵，每个数字代表的是一个单独像素的亮度。
在 RGB 模型中，彩色图片是由 3 个这样的矩阵组成的，每个矩阵对应着 3 个颜色通道（红、绿、蓝）中的一个，在黑白图像（专业说法为灰度模式）中，我们仅使用一个矩阵。每个矩阵都存储着 0 到 255 的数值。

#### 卷积

##### 二维卷积

我们有一个小的数字矩阵（称作卷积核或滤波器），我们将它传递到我们的图像上，然后基于卷积核的数值进行变换，后续的特征图的值要通过下面的公式计算：

$$
G(m,n)=(f*h)[m,n]=\sum_{i}\sum_{j}f(m-i,n-j)*h(i,j)
$$

其中，$f$为输入图像，$h$为卷积核，$m$和$n$为计算结果的行列索引。

为了解决每次卷积后图像都会收缩的问题，我们可以使用一个额外的边界来填充图像，填充后矩阵的维度为：

$$
n_{out}=\lfloor \frac{n_{in}+2p-f}{s}+1\rfloor
$$

其中，$p$为填充（padding）的宽度，$f$为卷积核的维度（通常是奇数），$s$为步长。

#### 立体卷积

#### 正向传播

#### 反向传播